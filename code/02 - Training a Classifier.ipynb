{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we will cover the basic concepts for training convolutional neural networks in Tensorflow. We will be specifically building a network to detect presence or absence of brain tumors from multimodal MR images. The data that we will be using in this tutorial comes from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/\n",
    "\n",
    "For basics of Tensorflow operation and neural networks, consider reviewing the first part of this series **01 - Introduction to Data, Tensorflow and Deep Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules\n",
    "\n",
    "To train our simple classifer implementation, we will require three open-source libraries (`tensorflow`, `numpy` and `os`) as well as our custom modules created for this tutorial (`net`, `data`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf, numpy as np\n",
    "import net, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter variables\n",
    "\n",
    "Hyperparameters are parameters whose values are set before the learning process begins and which in turn influence and direct the training process. These will be the three most important hyperparameter variables to vary in this experiment. We will cover these in more detail as they are encountered in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5000\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "Here we perform some basic preparatory steps including: \n",
    "\n",
    "* setting the data root directory (if you are not following on AWS, change `root = '../data'` to use local toy dataset in this repository)\n",
    "* making the output directory for saving training checkpoints and logs\n",
    "* defining an `ops` dictionary to save operations\n",
    "* reseting any existing graph that may exist (`tf.reset_default_graph()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/data/brats/npy'\n",
    "output_dir = '../exp_classifier' \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "ops = {}\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data batch\n",
    "\n",
    "A data **mini-batch** is used to describe the collection of image and label pairs used to perform one update of our network parameters. The more number of images and labels we use for each update, the more likely that update is to reflect the underlying population data. However, the trade-off is that computationally each network update will require more time. A good initial starting point for images matrices of our dataset may be 16 or 32. \n",
    "\n",
    "To implement batching, we will use a prepared template method `net.init_batch()` to load a number of slices simulatenously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = net.init_batch(batch_size, root='/data/brats/npy', one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "\n",
    "A tensorflow **placeholder** is an entry point for us to feed actual data values into the model. We must define this **placeholder** and all subsequent downstream operations performed on this **placeholder** before ever passing data into the model. \n",
    "\n",
    "The placeholder `X` will serve as the method for introduction image data into the graph. The placeholder `y` will serve as the method for introducing the correct target label representing presence (1) or absence (0) of tumor. The placeholder `mode` will serve as a method for introducing whether or not the graph is being executed for training or for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 240, 240, 4], name='X')\n",
    "y = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "mode = tf.placeholder(tf.bool, name='mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "In this example we will be using the architecture defined by `net.create_classifier()`. This classifer alternates a series of convolutions, ReLU non-linearities and max-pooling to collapse the `240 x 240 x 1` input into a single 2-channel out, one number representing likelihood of no tumor and the other representing likelihood of tumor.\n",
    "\n",
    "A simplified diagram of the architecture is as follows:\n",
    "```\n",
    "DEFINITION\n",
    "----------\n",
    "BLOCK = [ CONV --> RELU --> CONV --> RELU --> POOL ]\n",
    "\n",
    "LAYER NAME | OUTPUT SHAPE\n",
    "------------------------------\n",
    "BLOCK-01   | 120 x 120 x 8\n",
    "BLOCK-02   | 060 x 060 x 16 \n",
    "BLOCK-03   | 030 x 030 x 32\n",
    "BLOCK-04   | 015 x 015 x 64\n",
    "BLOCK-05   | 007 x 007 x 96\n",
    "BLOCK-06   | 003 x 003 x 128\n",
    "FLATTEN    | 001 x 001 x 1152\n",
    "FC         | 001 x 001 x 2\n",
    "```\n",
    "\n",
    "Note that this diagram is simplified. This particular algorithm also implements minor additions such as batch normalization and L2 regularization which are byeond the scope of this tutorial. See source code for further information. \n",
    "\n",
    "To implement this architecture, simply call the `net.create_classifier()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net.create_classifier(X, training=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and error\n",
    "\n",
    "Next, based on these prediction logits, we need to give the algorithm feedback whether or not the network is correct. To do so, we will use the softmax function, a formula that computes the exponential (e-power) of the given input value divided by the sum of exponential values of all the values in the inputs. For a classification model these values on the range of (0, 1) together represent the probability distribution of the different label classes.\n",
    "\n",
    "To implement a softmax function, we will use the a prepared template function `net.loss_sce()` that serve as a wrapper to the underlying `tf.nn.sparse_softmax_cross_entropy_with_logits()` method described in the previous tutorial.\n",
    "\n",
    "In addition to the loss function, we want to gauge how accurate (%) the predictions are in a human-interpretable way. To do, we will keep track of the `top-k` accuracy of our model, which in our simple two-class prediction simplifies to a `top-1` score (e.g. `k=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "losses['sce'] = net.loss_sce(pred, y)\n",
    "losses['topk'] = net.error_topk(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "An optimizer is a strategy used to update the network parameters through backprogration by taking into account the quantitative loss function. We will be using the Adam optimizer for our tutorials, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. For further reading, see the following link for the original paper: https://arxiv.org/abs/1412.6980\n",
    "\n",
    "A key hyperparameter here is the optimizer **learning rate**. The learning rate describes the absolute magnitude of update for each parameter for one iteration. A higher learning rate will result in a correspondingly larger, more aggresive \"step\" towards the global minimum of a function, however a learning rate that is too high may cause the network to overshoot the true function minimum and even worse, may lead to network instability. A good initial learning rate to use in most experiments, without other guiding heuristics, is `1e-3` which is what we will set our initial `learning_rate` hyperparameter to.\n",
    "\n",
    "Note that the `tf.control_dependencies()` method here ensures that any other pending graph operations must be complete before the optimizer node is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    ops['train'] = optimizer.minimize(losses['sce'], global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections\n",
    "\n",
    "After creating the placeholders and predictions, we will add them to named Graph collections for easy retrieval after training is complete during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.add_to_collection(\"inputs\", X)\n",
    "tf.add_to_collection(\"inputs\", mode)\n",
    "tf.add_to_collection(\"outputs\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "TensorBoard is useful utility that can be used to track various statistics during the network training process. Here we set up operations to create log files that can be loaded using the TensorBoard interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.histogram('logits', pred)\n",
    "tf.summary.scalar('sce', losses['sce'])\n",
    "tf.summary.scalar('topk', losses['topk'])\n",
    "ops['summary'] = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training\n",
    "\n",
    "Now that graph, loss function and optimizer have been configured, it is time to run the training algorithm. To begin we define a new `tf.Session` class and initialize our basic objects to enable saving intermediate checkpoints and writing log data. In addition we initialize `coord` and `thread` objects to handle asynchronous loading of input data into batches:\n",
    "```\n",
    "sess, saver, writer_train, writer_valid = net.init_session(sess, output_dir)\n",
    "```\n",
    "\n",
    "To perform actual training, we will construct a loop to repeat parameter updates a total of `iteration` times. For each update, we will start by loading the data into batches `X_batch` and `y_batch`:\n",
    "```\n",
    "X_batch, y_batch = sess.run([batch['train']['X'], batch['train']['y']])\n",
    "```\n",
    "\n",
    "We will then collapse the label masks into either 0 or 1 based on whether or not any mask is present in the entire label:\n",
    "```\n",
    "y_batch = np.max(y_batch > 0, axis=(1,2)).astype('float32')\n",
    "```\n",
    "\n",
    "At last we call `sess.run()` to run one iteration of the training process. Specifically we wil request the network to output the `error` (accuracy %), `summary` (used for creating logs) and `step` (global step reflecting total number of iterations). Note that the `ops['train']` operation corresponding to the optimizer node is also called, but there is no output for this function and hence no (`_,`) return variable.\n",
    "```\n",
    " _, metric, summary, step  = sess.run(\n",
    "                [ops['train'], losses, ops['summary'], global_step],\n",
    "                feed_dict={\n",
    "                    X: X_batch, \n",
    "                    y: y_batch, \n",
    "                    mode: True})\n",
    "```\n",
    "\n",
    "Finally, for every 10 updates, will ask the network to run against a separate validation cohort (e.g. completely separate from the training dataset) to track the overall generalization of the algorithm's learned representation:\n",
    "```\n",
    "if not i % 10:\n",
    "    ...\n",
    "```\n",
    "\n",
    "This entire training process can be executed by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../exp_classifier/checkpoint/model.ckpy\n",
      "\n",
      "\n",
      "Training Statistics:\n",
      "\n",
      "0004908 | TRAIN - sce : 0.1396 - topk : 0.9422  | VALID - sce : 0.3224 - topk : 0.8656 \r"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess, saver, writer_train, writer_valid = net.init_session(sess, output_dir)\n",
    "    print('\\n\\nTraining Statistics:\\n')\n",
    "\n",
    "    try:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        metrics = net.init_metrics(losses)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # --- Run a single iteration of training\n",
    "            X_batch, y_batch = sess.run([batch['train']['X'], batch['train']['y']])\n",
    "            y_batch = np.max(y_batch > 0, axis=(1,2)).astype('float32')\n",
    "            _, metric, summary, step  = sess.run(\n",
    "                [ops['train'], losses, ops['summary'], global_step],\n",
    "                feed_dict={\n",
    "                    X: X_batch, \n",
    "                    y: y_batch, \n",
    "                    mode: True})\n",
    "\n",
    "            writer_train.add_summary(summary, step)\n",
    "            metrics = net.update_ema(metrics, metric, mode='train', iteration=i)\n",
    "            net.print_status(metrics, step, metric_names=['sce', 'topk'])\n",
    "\n",
    "            # --- Every 10th iteration run a single validation batch\n",
    "            if not i % 10:\n",
    "\n",
    "                X_batch, y_batch = sess.run([batch['valid']['X'], batch['valid']['y']])\n",
    "                y_batch = np.max(y_batch > 0, axis=(1,2)).astype('float32')\n",
    "                metric, summary = sess.run(\n",
    "                    [losses, ops['summary']],\n",
    "                    feed_dict={\n",
    "                        X: X_batch, \n",
    "                        y: y_batch, \n",
    "                        mode: False})\n",
    "\n",
    "                writer_valid.add_summary(summary, step)\n",
    "                metrics = net.update_ema(metrics, metric, mode='valid', iteration=i)\n",
    "                net.print_status(metrics, step, metric_names=['sce', 'topk'])\n",
    "\n",
    "        saver.save(sess, '%s/checkpoint/model.ckpy' % output_dir)\n",
    "\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        saver.save(sess, '%s/checkpoint/model.ckpy' % output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above space you will see updates of algorithm training status including number of iterations and errors on both the training and validation set data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard \n",
    "\n",
    "### Overview\n",
    "\n",
    "TensorBoard is a suite of web applications for inspecting and understanding your TensorFlow runs and graphs. To use Tensorboard, you must embed specialized `tf.summary.*` operations into your graph which produce serialized protobufs that track various training statistics over time. The supported summary ops include:\n",
    "\n",
    "* tf.summary.scalar\n",
    "* tf.summary.image\n",
    "* tf.summary.audio\n",
    "* tf.summary.text\n",
    "* tf.summary.histogram\n",
    "\n",
    "During the training process, a specialized `summary.FileWriters()` class is used to take summary data created by `tf.summary.*` operations and write them to a specified directory, known as the `logdir`. This was implemented in following line of code above:\n",
    "```\n",
    "writer_valid.add_summary(summary, step)\n",
    "```\n",
    "\n",
    "### Launching TensorBoard\n",
    "\n",
    "To launch TensorBoard, go to the command prompt (either `Jupyter Home` > `New` > `Terminal` or return to terminal used to launch your SSH session) and type in the following command:\n",
    "```\n",
    "tensorboard --logdir=~/python/dl_tutorial/exp_classifier\n",
    "```\n",
    "\n",
    "Then open up a new tab in your browser and type in the following address pattern:\n",
    "```\n",
    "[IP-address]:6006\n",
    "```\n",
    "Where `[IP-address]` is the same address of form `xxx.xxx.xxx.xxx` that represents the IP address of your AWS instance. It should be the same prefix as your Jupyter notebook in the address bar currently at the top of your screen. For example, if the IP address is `34.215.158.68`, then the URL entered into the web browser is `34.215.158.68:6006`.\n",
    "\n",
    "For more information about TensorBoard usage, see link: https://github.com/tensorflow/tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final thoughts\n",
    "\n",
    "Feel free to continue training the algorithm until convergence at reasonable accuracy. Once complete, turn off the kernel (top menu > `Kernel` > `Shutdown`; you can keep this tab open in your browser to retrain later) so that it's resources can be used in the next notebook. You are now ready to move on the **03 - Inference with a Classifier** to use the newly trained network on data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
