{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we will cover the basic concepts for training a specific convolutional neural network architecture known as a U-net in TensorFlow. Using this architecture we are able to estimate boundaries of brain tumor tissue types (e.g. segmentation) from multimodal MR images. The data that we will be using in this tutorial comes from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/\n",
    "\n",
    "For basics of Tensorflow operation, neural networks and training, consider reviewing the preceding lectures in this series:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **01 - Introduction to Data, Tensorflow and Deep Learning** <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **02 - Training a Classifier** <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **03 - Inference with a Classifier**\n",
    "\n",
    "### U-net\n",
    "\n",
    "The U-net is a popular network design under the family of encoder-decoder architectures. In this type of architecture, an encoding (collapsing) arm gradually reduces the spatial dimension with pooling layers (or strided convolutions) while a decoding (expanding) arm gradually recovers the object details and spatial dimension. There are usually shortcut connections from encoder to decoder to help decoder recover the object details better. For further reading, see link for original paper (https://arxiv.org/abs/1505.04597) as well as a blog here about different segmentation techniques (http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules\n",
    "\n",
    "To train our simple classifer implementation, we will require three open-source libraries (`tensorflow`, `numpy` and `os`) as well as our custom modules created for this tutorial (`net`, `data`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter variables\n",
    "\n",
    "Hyperparameters are parameters whose values are set before the learning process begins and which in turn influence and direct the training process. These will be the three most important hyperparameter variables to vary in this experiment. We will cover these in more detail as they are encountered in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 2000\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "Here we perform some basic preparatory steps including making the output directory for saving training outputs, defining an `ops` dictionary to save operations, and reseting any existing graph that may exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../exp_unet' \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "ops = {}\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data batch\n",
    "\n",
    "A data **mini-batch** is used to describe the collection of image and label pairs used to perform one update of our network parameters. The more number of images and labels we use for each update, the more likely that update is to reflect the underlying population data. However, the trade-off is that computationally each network update will require more time. A good initial starting point for images matrices of our dataset may be 16 or 32. \n",
    "\n",
    "To implement batching, we will use a prepared template method `net.init_batch()` to load a number of slices simulatenously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = net.init_batch(batch_size, root='../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "\n",
    "A tensorflow **placeholder** is an entry point for us to feed actual data values into the model. We must define this **placeholder** and all subsequent downstream operations performed on this **placeholder** before ever passing data into the model. \n",
    "\n",
    "The placeholder `X` will serve as the method for introduction image data into the graph. The placeholder `y` will serve as the method for introducing the correct target label at each voxel location:\n",
    "```\n",
    "0 = background (no tumor)\n",
    "1 = edema\n",
    "2 = non-enhancing tumor \n",
    "3 = necrosis\n",
    "4 = enhancing tumor\n",
    "```\n",
    "\n",
    "The placeholder `mode` will serve as a method for introducing whether or not the graph is being executed for training or for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 240, 240, 4], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, 240, 240, 5], name='y')\n",
    "mode = tf.placeholder(tf.bool, name='mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "In this example we will be using a template U-net created by the `net.create_unet()` method. The encoder arm of the architecture is similar to the classifer from earlier in this tutorial series, implemented by alternating series of convolutions, ReLU non-linearities and max-pooling. In addition a symmetric decoder arm of the architecture uses convolutional-transpose operations to gradually upsample the feature maps and recover high-frequency object details.\n",
    "\n",
    "To implement this architecture, simply call the `net.create_unet()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net.create_unet(X, training=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and error\n",
    "\n",
    "Next, based on these prediction logits, we need to give the algorithm feedback whether or not the network is correct. To do so, we will use a modified **Dice** score loss function. Comparision between the true formal definition and our modified approximation are shown here:\n",
    "```\n",
    "Dice (formal) = 2 x (y_pred UNION y_true) \n",
    "              -------------------------\n",
    "               | y_pred | + | y_true | \n",
    "\n",
    "Dice (approx) = 2 x (y_pred * y_true) + d \n",
    "              -------------------------\n",
    "              | y_pred | + | y_true | + d \n",
    "```\n",
    "\n",
    "Here *d* is small delta == 1e-7 added both to numerator/denominator to prevent division by zero. Note that the approximation is necessary because the true formal Dice score definition is non-differentiable.\n",
    "\n",
    "To implement a Dice score loss function, we will use the a prepared template function `net.loss_dice()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops['dice'] = net.loss_dice(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "An optimizer is a strategy used to update the network parameters through backprogration by taking into account the quantitative loss function. We will be using the Adam optimizer for our tutorials, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. For further reading, see the following link for the original paper: https://arxiv.org/abs/1412.6980\n",
    "\n",
    "A key hyperparameter here is the optimizer **learning rate**. The learning rate describes the absolute magnitude of update for each parameter for one iteration. A higher learning rate will result in a correspondingly larger, more aggresive \"step\" towards the global minimum of a function, however a learning rate that is too high may cause the network to overshoot the true function minimum and even worse, may lead to network instability. A good initial learning rate to use in most experiments, without other guiding heuristics, is `1e-3` which is what we will set our initial `learning_rate` hyperparameter to.\n",
    "\n",
    "Note that the `tf.control_dependencies()` method here ensures that any other pending graph operations must be complete before the optimizer node is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    ops['train'] = optimizer.minimize(-ops['dice'], global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections\n",
    "\n",
    "After creating the placeholders and predictions, we will add them to named Graph collections for easy retrieval after training is complete during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save key placeholders/operations for future reference\n",
    "tf.add_to_collection(\"inputs\", X)\n",
    "tf.add_to_collection(\"inputs\", mode)\n",
    "tf.add_to_collection(\"outputs\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "TensorBoard is useful utility that can be used to track various statistics during the network training process. Here we set up operations to create log files that can be loaded using the TensorBoard interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add data to TensorBoard\n",
    "tf.summary.histogram('softmax-scores', pred)\n",
    "tf.summary.scalar('dice', ops['dice'])\n",
    "ops['summary'] = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training\n",
    "\n",
    "Now that graph, loss function and optimizer have been configured, it is time to run the training algorithm. To begin we define a new `tf.Session` class and initialize our basic objects to enable saving intermediate checkpoints and writing log data. In addition we initialize `coord` and `thread` objects to handle asynchronous loading of input data into batches:\n",
    "```\n",
    "sess, saver, writer_train, writer_valid = net.init_session(sess, output_dir)\n",
    "```\n",
    "\n",
    "To perform actual training, we will construct a loop to repeat parameter updates a total of `iteration` times. For each update, we will start by loading the data into batches `X_batch` and `y_batch`:\n",
    "```\n",
    "X_batch, y_batch = sess.run([batch['train']['X'], batch['train']['y']])\n",
    "```\n",
    "\n",
    "Then we call `sess.run()` to run one iteration of the training process. Specifically we wil request the network to output the `error` (accuracy %), `summary` (used for creating logs) and `step` (global step reflecting total number of iterations). Note that the `ops['train']` operation corresponding to the optimizer node is also called, but there is no output for this function and hence no (`_,`) return variable.\n",
    "```\n",
    " _, error, summary, step  = sess.run(\n",
    "                [ops['train'], ops['dice'], ops['summary'], global_step],\n",
    "                feed_dict={\n",
    "                    X: X_batch, \n",
    "                    y: y_batch, \n",
    "                    mode: True})\n",
    "```\n",
    "\n",
    "Finally, for every 10 updates, will ask the network to run against a separate validation cohort (e.g. completely separate from the training dataset) to track the overall generalization of the algorithm's learned representation:\n",
    "```\n",
    "if not i % 10:\n",
    "    ...\n",
    "```\n",
    "\n",
    "This entire training process can be executed by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002000 | Dice (train) : 0.99345 | Dice (valid): 0.99386"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # --- Run graph\n",
    "    sess, saver, writer_train, writer_valid = net.init_session(sess, output_dir)\n",
    "\n",
    "    try:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        errors = {'train': 0, 'valid': 0}\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "            X_batch, y_batch = sess.run([batch['train']['X'], batch['train']['y']])\n",
    "            _, error, summary, step  = sess.run(\n",
    "                [ops['train'], ops['dice'], ops['summary'], global_step],\n",
    "                feed_dict={\n",
    "                    X: X_batch, \n",
    "                    y: y_batch, \n",
    "                    mode: True})\n",
    "\n",
    "            writer_train.add_summary(summary, step)\n",
    "            errors = net.update_ema(errors, error, mode='train', iteration=i)\n",
    "            net.print_status(errors, step)\n",
    "\n",
    "            # --- Every 10th iteration run a single validation batch\n",
    "            if not i % 10:\n",
    "\n",
    "                X_batch, y_batch = sess.run([batch['valid']['X'], batch['valid']['y']])\n",
    "                error, summary = sess.run(\n",
    "                    [ops['dice'], ops['summary']],\n",
    "                    feed_dict={\n",
    "                        X: X_batch, \n",
    "                        y: y_batch, \n",
    "                        mode: False})\n",
    "\n",
    "                writer_valid.add_summary(summary, step / 10)\n",
    "                errors = net.update_ema(errors, error, mode='valid', iteration=i)\n",
    "                net.print_status(errors, step)\n",
    "\n",
    "        saver.save(sess, '%s/checkpoint/model.ckpy' % output_dir)\n",
    "\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        saver.save(sess, '%s/checkpoint/model.ckpy' % output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above space you will see updates of algorithm training status including number of iterations and errors on both the training and validation set data.\n",
    "\n",
    "### Final thoughts\n",
    "\n",
    "Feel free to continue training the algorithm until convergence at reasonable accuracy. Once complete, turn off the kernel (top menu > `Kernel` > `Shutdown`; you can keep this tab open in your browser to retrain later) so that it's resources can be used in the next notebook. You are now ready to move on the **05 - Inference with a U-net** to use the newly trained network on data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
