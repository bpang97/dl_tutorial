{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we will cover the basic concepts for developing convolutional neural networks in Tensorflow and also introduce the data that we will be using in this tutorial from the MICCAI Brain Tumor Segmentation Challenge (BRaTS).\n",
    "\n",
    "* Dependencies and tools\n",
    "* BRATS MRI data\n",
    "* TensorFlow graph structure\n",
    "* Convolutions\n",
    "* Activation functions\n",
    "* Max-pool\n",
    "* Loss functions\n",
    "\n",
    "More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n",
    "\n",
    "Several key dependencies and tools are required to develop deep learning algorithms. Here we discuss the rationale for these including the chosen tools used in this tutorial (and preloaded into the AWS instance) as well as alternatives that exist.\n",
    "\n",
    "### GPU and drivers\n",
    "\n",
    "The driving force behind recent advances in deep learning research over the past several years is the computational optimizations for parallel matrix operations afforded by graphics processing units (GPUs). The vast majority of all research and associated frameworks have been optimized for NVIDIA GPU cards, and are thus form the basis for the hardware used here.\n",
    "\n",
    "The software interface to the underlying NVIDIA hardware is mediated through CUDA (language/API for programming on the graphics card) and cuDNN (a library for implementing neural networks using CUDA). While it is possible to build your own deep learning framework using CUDA and cuDNN, many mature community-supported deep learning frameworks built on CUDA/cuDNN already exist. \n",
    "\n",
    "### Deep learning frameworks\n",
    "\n",
    "The vast majority of these deep learning libraries are designed for Python including TensorFlow, Theano, Keras (high-level API running on top of TensorFlow or Theano), Pytorch and several others. In this tutorial we will use TensorFlow, currently one of the most popularly used frameworks developed and maintained by the Google Brain team. \n",
    "\n",
    "While Python is the most commonly used programming language for deep learning due to its larger, more well-developed and supported ecosystem for machine learning libraries, several other alternatives exist. For example TensorFlow does support APIs for R, Java, Go and C. Matlab also has several add-ons for deep learning, as well as an open-source library in MatConvNet (http://www.vlfeat.org/matconvnet/).\n",
    "\n",
    "### Version requirements\n",
    "\n",
    "Given the complex interplay of software libraries, each specific version has a required set of dependenices. For example, given the NVIDIA K80 GPU card on your AWS EC2 instance and the most recent TensorFlow 1.5 library, the specific NVIDIA driver version 390 is needed along with the CUDA 9.0/cuDNN 7.0 libaries. The specific library and driver versions loaded into AWS are listed below:\n",
    "\n",
    "* NVIDIA driver version 390\n",
    "* CUDA 9.0 API\n",
    "* cuDNN 7.0 libraries\n",
    "* Tensorflow 1.5\n",
    "* Python 3.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRATS MRI data\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "The goal of this data is to generate segmentation masks demarcating various brain tumor components from 3D MR imaging volumes. A helper module `data.py` is provided in this repository for loading the preprocessed MRI data located in the `/data/brats/npy` directory of your AWS EC2 instance. Additionally the included `utils.py` library contains several useful methods for inline visualization using `matplotlib`. Let us first import these modules here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "from utils import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code will demonstrate how to load a single random 2D training image and label pair from the training cohort and display it with the provided `imshow()` method. Feel free to re-run the following cell repeatedly (`ctrl + Enter`) to see a series of example images from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, lbl = data.load(mode='train', n=1)\n",
    "imshow(dat, title='Random slice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of the next several lines of demonstration, we will be using a specific image and label pair (rather than randomly selected) so that all participants have identical data moving forward. Let us load that sample now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat, lbl = data.load(sid='brats_tcia_pat234_0001', z=81)\n",
    "imshow(dat, title='sid: pat234 | slice: 81')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of data\n",
    "\n",
    "Let us get to know this data a little bit more. The input images in the variable `dat` are matrices of shape `1 x 240 x 240 x 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four channels represent four different input MRI modalities, as described below:\n",
    "\n",
    "```\n",
    "dat[..., 0] = T2\n",
    "dat[..., 1] = FLAIR\n",
    "dat[..., 2] = T1 precontrast\n",
    "dat[..., 3] = T1 postcontrast\n",
    "```\n",
    "\n",
    "To visualize these different modalities run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(dat[..., 0], title='T2')\n",
    "imshow(dat[..., 1], title='FLAIR')\n",
    "imshow(dat[..., 2], title='T1 precontrast')\n",
    "imshow(dat[..., 3], title='T1 postcontrast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of labels\n",
    "\n",
    "The output labels in the variable `lbl` an matrices of shape `1 x 240 x 240 x 1`. At each voxel location, there are one of 5 possible labels:\n",
    "\n",
    "```\n",
    "0 = background (no tumor)\n",
    "1 = edema\n",
    "2 = non-enhancing tumor \n",
    "3 = necrosis\n",
    "4 = enhancing tumor\n",
    "```\n",
    "\n",
    "Let us take a look at the label now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(lbl, title='Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the tumor is predominantly edema (blue) and enhancing tumor (yellow). We can visualize the overlap of the data and labels by passing both matrices to the `imshow()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(dat=dat, lbl=lbl, title='Tumor with label overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow dataflow graphs\n",
    "\n",
    "Tensorflow uses a **dataflow** graph to represent your computation in terms of the dependencies between individual operations. **Dataflow** is a common programming model for parallel computing. In a dataflow graph, the nodes represent units of computation, and the edges represent the data consumed or produced by a computation.\n",
    "\n",
    "For example, in a TensorFlow graph, the `tf.matmul()` operation would correspond to a single node with two incoming edges (the matrices to be multiplied) and one outgoing edge (the result of the multiplication).\n",
    "\n",
    "For further reading, see the following link: https://www.tensorflow.org/programmers_guide/graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "\n",
    "A tensorflow **placeholder** is an entry point for us to feed actual data values into the model. We must define this **placeholder** and all subsequent downstream operations performed on this **placeholder** before ever passing data into the model.\n",
    "\n",
    "To define a placeholder `X` for our input data, we use the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 240, 240, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first argument `tf.float32` defines the dtype of the input data. The second argument `[None, 240, 240, 4]` defines that the input matrix shape will be `240 x 240 x 4`, with the leading `None` indicating that an arbitrary number of potential images may be stacked along the first dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions\n",
    "\n",
    "A convolution is the process of adding each element of a matrix to its local neighbors, weighted by a **convolutional kernel**. For further reading, see the following link: http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\n",
    "\n",
    "To implement a convolutional operation in tensorflow, we will use the `tf.layers.conv2d()` method which simplifies access to the underlying `tf.nn.conv2d()` method. The latter allows for more disrete control of the convolutional operation, however is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = tf.layers.conv2d(X, filters=8, kernel_size=(3, 3), padding='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first argument `X` is the placeholder we defined above and input into the convolution. The second argument `filters=8` defines that the output matrix has 8 channels (filter banks). The third argument `kernel_size=(3,3)` defines that the convolutional filter being applied will be of size `3 x 3`. Finally the final argument `padding='same'` defines that the image will be padded so that the output matrix shape will match the input matrix shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Any arbitrary series of convolutions stacked upon one another can be composed into just a single underlying convolutional operation e.g. representing a single linear transformation. Instead richness of expression arising from the many convolutional layers of a CNN arise from the interposition of non-linearities or **activation functions** in alternating layers. An non-linear function can act as an activation function, however for the purposes of this tutorial we will just focus on the rectified linear unit (`ReLU`) function defined as:\n",
    "```\n",
    "ReLU(x) = max(x, 0)\n",
    "```\n",
    "\n",
    "The `ReLU` function is currently one of the most popular choices for modern CNN architectures. Note however that many other activation functions are possible, and come with associated advantages and disadvantages. For further reading, see the following link: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\n",
    "To implement a `ReLU` nonlinearity, we will use the `tf.nn.relu()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = tf.nn.relu(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first and only argument `conv` is the output of our convolutional operation above. Note that this is a parameter-less operation. The output will be a matrix of equal shape to the original `conv` matrix, will all negative values set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-pool\n",
    "\n",
    "The `pooling` operation is a form of non-linear down-sampling. Specifically max-pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum value. It is useful to:\n",
    "\n",
    "1. Reduce computation for upper layers\n",
    "2. Provide a form of translation invariance\n",
    "\n",
    "Note that for many modern CNN architectures, another popular alternative for down-sampling is to perform a strided convolution. This operation has the benefit of preserving flow of gradients during backpropogation however is beyond the scope of this tutorial. This technique was popularized the \"All Convolutional Net\" (*Springenberg et al*). For further reading, see the following link for the original paper: https://arxiv.org/pdf/1412.6806.pdf.\n",
    "\n",
    "To implement a max-pool operation, we will use the `tf.layers.max_pooling2d()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = tf.layers.max_pooling2d(relu, pool_size=(2, 2), strides=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a graph\n",
    "\n",
    "Now that have a small (toy) graph built, let us see what is necessary to pass data through it. To begin, we will first need to create a `tf.Session` class object. Tensorflow uses the `tf.Session` class to represent a connection between the client program---typically a Python program---and the C++ runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must randomly initialize all the parameters in our model. In our simple example graph, the only parameters will be contained in the convolutional filter kernel we used in the first operation. Nonetheless, we will use a general `tf.global_variables_initializer()` operation, which can be used to efficiently initialize all parameters in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph has been created and all parameters initialized, we can proceed with passing data through the graph structure. To do so, we use the `tf.sess.run()` method, which has two arguments. The first argument is a single (or a list of several) output(s) we wish to extract from the graph. The second argument is a feed `dict` that matches defined placeholder with the appropriate data to place into the graph. In the following example, we will be passing the variable (image) `dat` into the `X` placeholder and recieving the result of `pool` on the other side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sess.run(pool, feed_dict={X: dat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess what the `output` matrix shape will be? (Hint: we performed a `conv` with `3 x 3 x 4 x 8` convolutional kernel followed by a `relu` non-linearity followed by a `2 x 2 max-pooling` operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you answer correctly? To understand what happened, the `conv` operation maps our 4-channel input into 8 different feature maps. The `relu` operation introduces non-linearity into our function however does not change the output layer size. Finally the `2 x 2 max-pool` operation downsamples by collapsing each 2x2 patch in our feature map into a single maximal value, thus decreasing the feature map size by 2 along both the height and width of the image.\n",
    "\n",
    "Feel free to visualize these intermediate feature maps (`output[..., 0]` will contain the first feature map, `output[..., 1]` the second, etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(output[..., 0], title='First feature map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, since we've only proceed with a minimal series of three operations, each of the feature maps at this early stage will look very similar to the original image with only some slight differences.\n",
    "\n",
    "Note that although our simple graph is a linear series of just three operations, graphs can become highly complex and non-linear in nature; as a result the output(s) that you wish to extract from the graph can be any intermediate operation at any point along your defined graph structure. Although in this example we requested the final node of our graph (`pool`) we could have replaced this argument with any others that we defined (e.g. `conv` or `relu` for example) if we wished instead to extract the output of another intermediate layer. Similarly the feed `dict` needs only to be filled with the dependent placeholders in your graph required for computing your requested output. \n",
    "\n",
    "### Closing a tf.session()\n",
    "\n",
    "Finally, because each active `tf.Session` class owns physical resources (such as GPUs and network connections), it is important to explicitly call `tf.Session.close()` when you are finished with it to free the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Now that we have working understanding of the basic building blocks of a neural network, let us put together a working example. Our goal will be a simple CNN classifier to determine whether or not slice contains tumor. To begin we will reset the previous tensorflow graph and start by defining the requisite placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 240, 240, 4], name='X')\n",
    "y = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "mode = tf.placeholder(tf.bool, name='mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the placeholder `X` for the input data, we also define two additional newplaceholders, `y` and `mode`. The placeholder `y` will serve as the method for introducing the correct target label representing presence (1) or absence (0) of tumor. The placeholder `mode` will serve as a method for introducing whether or not the graph is being executed for training or for validation. \n",
    "\n",
    "Now we will build a much larger graph. The `net.py` module included in this tutorial contains several simple templated architectures. Let us start by importing this as well as the Numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import net, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will be using the architecture defined by `net.create_classifier()`. This classifer alternates a series of convolutions, ReLU non-linearities and max-pooling to collapse the `240 x 240 x 1` input into a single 2-channel out, one number representing likelihood of no tumor and the other representing likelihood of tumor.\n",
    "\n",
    "A simplified diagram of the architecture is as follows:\n",
    "```\n",
    "DEFINITION\n",
    "----------\n",
    "BLOCK = [ CONV --> RELU --> CONV --> RELU --> POOL ]\n",
    "\n",
    "LAYER NAME | OUTPUT SHAPE\n",
    "------------------------------\n",
    "BLOCK-01   | 120 x 120 x 8\n",
    "BLOCK-02   | 060 x 060 x 16 \n",
    "BLOCK-03   | 030 x 030 x 32\n",
    "BLOCK-04   | 015 x 015 x 64\n",
    "BLOCK-05   | 007 x 007 x 96\n",
    "BLOCK-06   | 003 x 003 x 128\n",
    "FLATTEN    | 001 x 001 x 1152\n",
    "FC         | 001 x 001 x 2\n",
    "```\n",
    "\n",
    "Note that this diagram is simplified. This particular algorithm also implements minor additions such as batch normalization and L2 regularization which are byeond the scope of this tutorial. See source code for further information. \n",
    "\n",
    "To implement this architecture, simply call the `net.create_classifier()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net.create_classifier(X, training=mode)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind from the shape output above that `pred` is a two-value arary that represents a likelihood score: the first value represents how strongly the network believes there is no tumor, the second value represents how strongly the network believes there is a tumor. Thus `pred[0] >> pred[1]` indicates a prediction of no tumor, while `pred[0] << pred[1]` indicates a prediction of tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Next, based on these prediction logits, we need to give the algorithm feedback whether or not the network is correct. To do so, we need to define a specific function that gauges overall algorithm accuracy. Specifically we will use the softmax function, a formula that computes the exponential (e-power) of the given input value divided by the sum of exponential values of all the values in the inputs. For a classification model these values on the range of (0, 1) together represent the probability distribution of the different label classes.\n",
    "\n",
    "To implement a softmax function, we will use the `tf.nn.sparse_softmax_cross_entropy_with_logits()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the graph\n",
    "\n",
    "Let us now run this new, expanded graph architecture. We will begin by creating the `tf.Session` object and initializing all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test the network by passing pairs of data and labels into the graph and checking the reported network predictions (`pred`) and loss value (`loss`). \n",
    "Feel free to re-run the following cell repeatedly (`ctrl + Enter`) to see the network in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, lbl = data.load(mode='train', n=1)\n",
    "lbl = np.max(lbl > 0, axis=(1,2,3)).astype('int32')\n",
    "sce, logits = sess.run([loss, pred], {X: dat, y: lbl, mode: True})\n",
    "\n",
    "print('REAL (tumor, y/n?): %s' % ('y' if lbl else 'n'))\n",
    "print('PRED (tumor, y/n?): %s' % ('y' if logits[0, 1] > logits[0, 0] else 'n'))\n",
    "print('LOSS (lower value is better prediction): %f' % sce)\n",
    "\n",
    "imshow(dat[..., 1], title='FLAIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, you've done it! We've created a formidible network architecture that is able to map any given `240 x 240 x 4` brain MRI slice of data into predictions regarding presence/absence of tumor. Furthermore we have a loss function that quantitatively defines how accurate the predictions are. Of course at this point, the network is simply making arbitrary guesses because it's internal parameters have just been initalized to random values. However we now have all the requisite foundation to perform *neural network training*, a process by which the algorithm on its own will learn to readjust this parameters to learn how to generate accurate predictions. This process is what we will cover in **Part 02 - Training a Classifier** of this series.\n",
    "\n",
    "Before we leave, go ahead and close our `tf.Session` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
